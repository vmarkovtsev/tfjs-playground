{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip\n",
    "!unzip Sentiment-Analysis-Dataset.zip\n",
    "!rm Sentiment-Analysis-Dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T18:56:32.803485",
     "start_time": "2018-11-07T17:56:28.922Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cpu backend was already registered. Reusing existing backend\n"
     ]
    }
   ],
   "source": [
    "const fs = require('fs');\n",
    "const csv_parse = require('csv-parse');\n",
    "const pp = require('promisepipe');\n",
    "const tf = require('@tensorflow/tfjs');\n",
    "require('@tensorflow/tfjs-node-gpu'); undefined;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T18:46:37.810689",
     "start_time": "2018-11-07T17:46:37.803Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorflow'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.getBackend()  // should be \"tensorflow\", not \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T18:56:34.358979",
     "start_time": "2018-11-07T17:56:34.325Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextsLoader {\n",
    "    constructor(config) {\n",
    "        const maxLength = config.maxTextLength;\n",
    "        const trainBatchSize = config.trainBatchSize;\n",
    "        const validBatchSize = config.validBatchSize;\n",
    "        const validRatio = config.validRatio;\n",
    "\n",
    "        this.trainSet = null;\n",
    "        this.trainPos = 0;\n",
    "        this.trainBatchX = tf.buffer([trainBatchSize, maxLength], 'int32');\n",
    "        this.trainBatchY = tf.buffer([trainBatchSize, 2], 'float32');\n",
    "        this.validSet = null;\n",
    "        this.validPos = 0;\n",
    "        this.validBatchX = tf.buffer([validBatchSize, maxLength], 'int32');\n",
    "        this.validBatchY = tf.buffer([validBatchSize, 2], 'float32');\n",
    "        this.validRatio = validRatio;\n",
    "        this.minOrd = 0;\n",
    "        this.maxOrd = 256;\n",
    "        this.X = [];\n",
    "        this.Y = [];\n",
    "    }\n",
    "    \n",
    "    maxTextLength() {\n",
    "        return this.trainBatchX.shape[1];\n",
    "    }\n",
    "    \n",
    "    trainBatchSize() {\n",
    "        return this.trainBatchX.shape[0];\n",
    "    }\n",
    "    \n",
    "    validBatchSize() {\n",
    "        return this.validBatchX.shape[0];\n",
    "    }\n",
    "    \n",
    "    async load(csvPath) {\n",
    "        const maxLength = this.maxTextLength();\n",
    "        let minOrd = 256;\n",
    "        let maxOrd = 0;\n",
    "        this.X = [];\n",
    "        this.Y = [];\n",
    "        let trueCount = 0;\n",
    "        await pp(fs.createReadStream(csvPath)\n",
    "          .pipe(csv_parse({skip_lines_with_error: true, columns: true}))\n",
    "          .on(\"data\", (chunk) => {\n",
    "            const text = unescape(encodeURIComponent(chunk.SentimentText.trim()));\n",
    "            const len = Math.min(text.length, maxLength);\n",
    "            const arr = new Uint8Array(len);\n",
    "            for (let i = 0; i < len; i++) {\n",
    "                const ord = text.charCodeAt(i);\n",
    "                if (ord > maxOrd) {\n",
    "                    maxOrd = ord;\n",
    "                }\n",
    "                if (ord < minOrd) {\n",
    "                    minOrd = ord;\n",
    "                }\n",
    "                arr[i] = ord;\n",
    "            }\n",
    "            const label = parseInt(chunk.Sentiment);\n",
    "            if (label) {\n",
    "                trueCount++;\n",
    "            }\n",
    "            this.X.push(arr);\n",
    "            this.Y.push(label);\n",
    "          }));\n",
    "        this.minOrd = minOrd;\n",
    "        this.maxOrd = maxOrd;\n",
    "        console.log(\"char range:\", minOrd, maxOrd);\n",
    "        const permutation = await tf.range(0, this.X.length, 1, 'int32').data();\n",
    "        await TextsLoader.shuffle(permutation);\n",
    "        const cutoff = Math.floor(this.validRatio * this.X.length);\n",
    "        this.trainSet = permutation.slice(cutoff, this.X.length);\n",
    "        this.validSet = permutation.slice(0, cutoff);\n",
    "        console.log('train:', this.trainSet.length, 'samples,', this.countTrainBatches(1), 'batches');\n",
    "        console.log('validation:', this.validSet.length, 'samples,', this.countValidBatches(1), 'batches');\n",
    "        console.log('balance:', trueCount / this.X.length);\n",
    "        return this;\n",
    "    }\n",
    "    \n",
    "    charRange() {\n",
    "        return this.maxOrd + 1 - (this.minOrd - 1);\n",
    "    }\n",
    "    \n",
    "    size() {\n",
    "        return this.X.length;\n",
    "    }\n",
    "    \n",
    "    countTrainBatches(numEpochs) {\n",
    "        const batchesPerEpoch = Math.floor(this.trainSet.length / this.trainBatchSize());\n",
    "        return Math.floor(numEpochs * batchesPerEpoch);\n",
    "    }\n",
    "    \n",
    "    countValidBatches(numEpochs) {\n",
    "        const batchesPerEpoch = Math.floor(this.validSet.length / this.validBatchSize());\n",
    "        return Math.floor(numEpochs * batchesPerEpoch);\n",
    "    }\n",
    "    \n",
    "    async nextTrainBatch() {\n",
    "        return await this.nextBatch('train');\n",
    "    }\n",
    "    \n",
    "    async nextValidBatch() {\n",
    "        return await this.nextBatch('valid');\n",
    "    }\n",
    "    \n",
    "    async nextBatch(name) {\n",
    "        const X = this[name + 'BatchX'];\n",
    "        const Y = this[name + 'BatchY'];\n",
    "        const set = this[name + 'Set'];\n",
    "        const posName = name + 'Pos';\n",
    "        const batchSize = X.shape[0];\n",
    "        const maxLength = Y.shape[1];\n",
    "        const offset = this.minOrd - 1;\n",
    "        if (this[posName] + batchSize > set.length) {\n",
    "            await TextsLoader.shuffle(set);\n",
    "            this[posName] = 0;\n",
    "        }\n",
    "        for (let i = 0; i < batchSize; i++) {\n",
    "            const pos = set[i + this[posName]];\n",
    "            const text = this.X[pos];\n",
    "            for (let j = 0; j < text.length; j++) {\n",
    "                X.set(text[j] - offset, i, j);\n",
    "            }\n",
    "            for (let j = text.length; j < maxLength; j++) {\n",
    "                X.set(0, i, j);\n",
    "            }\n",
    "            const label = this.Y[pos];\n",
    "            Y.set(label, i, label);\n",
    "            Y.set(0, i, 1 - label);\n",
    "        }\n",
    "        this[posName] += batchSize;\n",
    "        return [X.toTensor(), Y.toTensor()];\n",
    "    }\n",
    "    \n",
    "    toBatch(text) {\n",
    "        const maxLength = this.maxTextLength();\n",
    "        const utf8text = unescape(encodeURIComponent(text.trim()));\n",
    "        const len = Math.min(utf8text.length, maxLength);\n",
    "        const arr = tf.buffer([1, maxLength], 'int32');\n",
    "        const offset = this.minOrd - 1;\n",
    "        for (let i = 0; i < len; i++) {\n",
    "            const ord = utf8text.charCodeAt(i);\n",
    "            arr.set(ord - offset, 0, i);\n",
    "        }\n",
    "        return arr.toTensor();\n",
    "    }\n",
    "    \n",
    "    static async shuffle(array) {\n",
    "        // Fisher-Yates\n",
    "        const entropy = await tf.randomUniform([array.length], 0, 1).data(); \n",
    "        for (let i = array.length - 1; i > 0; i--) {\n",
    "            let swapped = Math.floor(entropy[i] * (i + 1));\n",
    "            let tmp = array[i];\n",
    "            array[i] = array[swapped];\n",
    "            array[swapped] = tmp;\n",
    "        }\n",
    "        return array;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T18:57:40.648592",
     "start_time": "2018-11-07T17:56:39.191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char range: 9 226\n",
      "train: 1262850 samples, 12628 batches\n",
      "validation: 315712 samples, 631 batches\n",
      "balance: 0.5005505010256169\n"
     ]
    }
   ],
   "source": [
    "var loader = await (new TextsLoader({\n",
    "    maxTextLength: 140,\n",
    "    trainBatchSize: 100,\n",
    "    validBatchSize: 500,\n",
    "    validRatio: 0.2\n",
    "})).load('Sentiment Analysis Dataset.csv');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T18:57:40.666314",
     "start_time": "2018-11-07T17:56:41.774Z"
    }
   },
   "outputs": [],
   "source": [
    "function createModel(hyperp) {\n",
    "    hyperp = {\n",
    "        type: hyperp.type || \"classification\",\n",
    "        learning_rate: hyperp.learning_rate || 0.001,\n",
    "        lstm_units: hyperp.lstm_units || 96,\n",
    "        lstm_layers: hyperp.lstm_layers || 2,\n",
    "        bidi: hyperp.bidi || false,\n",
    "        attention: hyperp.attention || false,\n",
    "        dropout: hyperp.dropout || 0,\n",
    "    }\n",
    "\n",
    "    // uint8 dtype is not supported yet\n",
    "    let input = tf.input({shape: [loader.maxTextLength()], dtype: \"int32\"});\n",
    "    let head = tf.layers.embedding({inputDim: loader.charRange(),\n",
    "                                    outputDim: loader.charRange(),\n",
    "                                    embeddingsInitializer: \"identity\",\n",
    "                                    trainable: false}).apply(input);\n",
    "    for (let i = 0; i < hyperp.lstm_layers; i++) {\n",
    "        let lstm = tf.layers.lstm({\n",
    "            units: hyperp.lstm_units,\n",
    "            unitForgetBias: true,\n",
    "            returnSequences: true,\n",
    "        });\n",
    "        if (hyperp.bidi) {\n",
    "            head = tf.layers.bidirectional({layer: lstm, mergeMode: 'concat'}).apply(head);\n",
    "            if (i < hyperp.lstm_layers - 1 && hyperp.dropout > 0) {\n",
    "                head = tf.layers.dropout({\"rate\": hyperp.dropout, \"seed\": 7}).apply(head);\n",
    "            }\n",
    "        } else {\n",
    "            head = lstm.apply(head);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if (hyperp.attention && hyperp.type != 'charnn') {\n",
    "        attention = tf.layers.dense({units: 1, activation: 'tanh'}).apply(head);\n",
    "        attention = tf.layers.flatten().apply(attention);\n",
    "        attention = tf.layers.activation('softmax').apply(attention);\n",
    "        attention = tf.layers.repeatVector({n: hyperp.lstm_units * (hyperp.bidi + 1)}).apply(attention);\n",
    "        attention = tf.layers.permute({dims: [2, 1]}).apply(attention);\n",
    "        head = tf.layers.multiply().apply([head, attention]);\n",
    "        head = tf.layers.globalAveragePooling1d({}).apply(head);\n",
    "    }\n",
    "    let loss;\n",
    "    if (hyperp.type == 'charnn') {\n",
    "        head = tf.layers.timeDistributed({\n",
    "            layer: tf.layers.dense({units: loader.charRange(), activation: 'softmax'})\n",
    "        }).apply(head);\n",
    "        loss = 'categoricalCrossentropy';\n",
    "    } else if (hyperp.type == 'classification') {\n",
    "        if (!hyperp.attention) {\n",
    "            head = tf.layers.flatten().apply(head);\n",
    "        }\n",
    "        head = tf.layers.dense({units: 2, activation: 'softmax'}).apply(head);\n",
    "        // two classes; change to 'categoricalCrossentropy' if > 2\n",
    "        loss = 'binaryCrossentropy';\n",
    "    }\n",
    "    let model = tf.model({inputs: input, outputs: head})\n",
    "    model.compile({optimizer: tf.train.adam(hyperp.learning_rate), loss: loss, metrics: ['accuracy']});\n",
    "    model.summary();\n",
    "    return model;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T18:49:41.212634",
     "start_time": "2018-11-07T17:49:41.204Z"
    }
   },
   "outputs": [],
   "source": [
    "async function trainCharnn(model, epochs, validationInterval) {\n",
    "    validationInterval = validationInterval || 50;\n",
    "    \n",
    "    var batchesCount = loader.countTrainBatches(epochs);\n",
    "    for (let i = 0; i < batchesCount; i++) {\n",
    "        let validBatch;\n",
    "        if (i % validationInterval === 0) {\n",
    "            validBatch = await loader.nextValidBatch();\n",
    "            validBatch[1].dispose();\n",
    "            validBatch[1] = tf.oneHot(validBatch[0], loader.charRange());\n",
    "        }\n",
    "\n",
    "        const trainBatch = await loader.nextTrainBatch();\n",
    "        trainBatch[1].dispose();\n",
    "        trainBatch[1] = tf.oneHot(trainBatch[0], loader.charRange());\n",
    "        const result = await model.fit(trainBatch[0], trainBatch[1], {\n",
    "            batchSize: loader.trainBatchSize(),\n",
    "            epochs: 1,\n",
    "            validationData: validBatch,\n",
    "            shuffle: false\n",
    "        });\n",
    "        tf.dispose(trainBatch);\n",
    "\n",
    "        if (i % validationInterval === 0) {\n",
    "            tf.dispose(validBatch);\n",
    "            const loss = result.history.loss[0];\n",
    "            const accuracy = result.history.acc[0];\n",
    "            console.log((i + 1) + '/' + batchesCount, 'loss:', loss, '   accuracy:', accuracy);\n",
    "            const text = \"Awesome, wonderful evening in the prison!\";\n",
    "            const predBatch = loader.toBatch(text);\n",
    "            const out = model.predict(predBatch).buffer();\n",
    "            predBatch.dispose();\n",
    "            let predText = \"\";\n",
    "            for (let i = 0; i < text.length; i++) {\n",
    "                let maxP = 0;\n",
    "                let maxJ = -1;\n",
    "                for (let j = 0; j < loader.charRange(); j++) {\n",
    "                    const p = out.get(0, i, j);\n",
    "                    if (p > maxP) {\n",
    "                        maxP = p;\n",
    "                        maxJ = j;\n",
    "                    }\n",
    "                }\n",
    "                maxJ += loader.minOrd - 1;\n",
    "                predText += String.fromCharCode(maxJ);\n",
    "            }\n",
    "            console.log(predText);\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-07T17:51:27.738Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Orthogonal initializer is being called on a matrix with more than 2000 (36864) elements: Slowness may result.\n",
      "Orthogonal initializer is being called on a matrix with more than 2000 (36864) elements: Slowness may result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output shape              Param #   \n",
      "=================================================================\n",
      "input1 (InputLayer)          [null,140]                0         \n",
      "_________________________________________________________________\n",
      "embedding_Embedding1 (Embedd [null,140,219]            47961     \n",
      "_________________________________________________________________\n",
      "lstm_LSTM1 (LSTM)            [null,140,96]             121344    \n",
      "_________________________________________________________________\n",
      "lstm_LSTM2 (LSTM)            [null,140,96]             74112     \n",
      "_________________________________________________________________\n",
      "time_distributed_TimeDistrib [null,140,219]            21243     \n",
      "=================================================================\n",
      "Total params: 264660\n",
      "Trainable params: 216699\n",
      "Non-trainable params: 47961\n",
      "_________________________________________________________________\n",
      "1/37884 loss: 5.396839618682861    accuracy: 0.0077142855152487755\n",
      "d\u001b                       \n"
     ]
    }
   ],
   "source": [
    "// Experiment 101 - predict the last character in the sequence.\n",
    "// This is a trivial task with an obvious information leak which\n",
    "// proves that all the plumbing works correctly.\n",
    "// We gradually turn on the features: bidi, attention.\n",
    "// Expected accuracy: 0.99(9).\n",
    "\n",
    "trainCharnn(createModel({'type': 'charnn'}), 3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Orthogonal initializer is being called on a matrix with more than 2000 (16384) elements: Slowness may result.\n",
      "Orthogonal initializer is being called on a matrix with more than 2000 (16384) elements: Slowness may result.\n",
      "Orthogonal initializer is being called on a matrix with more than 2000 (16384) elements: Slowness may result.\n",
      "Orthogonal initializer is being called on a matrix with more than 2000 (16384) elements: Slowness may result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output shape              Param #   \n",
      "=================================================================\n",
      "input1 (InputLayer)          [null,140]                0         \n",
      "_________________________________________________________________\n",
      "embedding_Embedding1 (Embedd [null,140,219]            47961     \n",
      "_________________________________________________________________\n",
      "bidirectional_Bidirectional1 [null,140,128]            145408    \n",
      "_________________________________________________________________\n",
      "bidirectional_Bidirectional2 [null,140,128]            98816     \n",
      "_________________________________________________________________\n",
      "time_distributed_TimeDistrib [null,140,219]            28251     \n",
      "=================================================================\n",
      "Total params: 320436\n",
      "Trainable params: 272475\n",
      "Non-trainable params: 47961\n",
      "_________________________________________________________________\n",
      "1/12628 loss: 5.386876106262207    accuracy: 0.0022857142612338066\n",
      "ttÅÅttttttÆÆÆ\n",
      "51/12628 loss: 3.26692533493042    accuracy: 0.16521427035331726\n",
      "                                         \n",
      "101/12628 loss: 3.22097110748291    accuracy: 0.17485713958740234\n",
      "@                                        \n",
      "151/12628 loss: 3.111598253250122    accuracy: 0.17135712504386902\n",
      "@                                        \n",
      "201/12628 loss: 2.7190043926239014    accuracy: 0.2507142722606659\n",
      "@aa           e                       o  \n",
      "251/12628 loss: 1.9405195713043213    accuracy: 0.6410714387893677\n",
      "@aasoaen oo aer to eeenini in the trisons\n",
      "301/12628 loss: 1.1369012594223022    accuracy: 0.8309999704360962\n",
      "@wesomen wonder ul eeening in the srisons\n",
      "351/12628 loss: 0.6029338836669922    accuracy: 0.9238570928573608\n",
      "awesome, wonder ul evening in the prison!\n",
      "401/12628 loss: 0.3952929973602295    accuracy: 0.9363570809364319\n",
      "awesome, wonderful evening in the prison!\n"
     ]
    }
   ],
   "source": [
    "trainCharnn(createModel({'type': 'charnn', 'bidi': true, 'lstm_units': 64}), 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T18:55:29.038780",
     "start_time": "2018-11-07T17:55:29.020Z"
    }
   },
   "outputs": [],
   "source": [
    "async function trainClassification(model, epochs, validationInterval) {\n",
    "    validationInterval = validationInterval || 50;\n",
    "    \n",
    "    var batchesCount = loader.countTrainBatches(epochs);\n",
    "    for (let i = 0; i < batchesCount; i++) {\n",
    "        let validBatch;\n",
    "        if (i % validationInterval === 0) {\n",
    "            validBatch = await loader.nextValidBatch();\n",
    "        }\n",
    "\n",
    "        const trainBatch = await loader.nextTrainBatch();\n",
    "        const result = await model.fit(trainBatch[0], trainBatch[1], {\n",
    "            batchSize: loader.trainBatchSize(),\n",
    "            epochs: 1,\n",
    "            validationData: validBatch,\n",
    "            shuffle: false\n",
    "        });\n",
    "        tf.dispose(trainBatch);\n",
    "\n",
    "        if (i % validationInterval === 0) {\n",
    "            tf.dispose(validBatch);\n",
    "            const loss = result.history.loss[0];\n",
    "            const accuracy = result.history.acc[0];\n",
    "            console.log((i + 1) + '/' + batchesCount, 'loss:', loss, '   accuracy:', accuracy);\n",
    "            const text = \"Awesome, wonderful evening in the prison!\";\n",
    "            const predBatch = loader.toBatch(text);\n",
    "            const out = model.predict(predBatch).buffer();\n",
    "            predBatch.dispose();\n",
    "            console.log(\"negative:\", out.get(0, 0), \"   positive:\", out.get(0, 1));\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Orthogonal initializer is being called on a matrix with more than 2000 (16384) elements: Slowness may result.\n",
      "Orthogonal initializer is being called on a matrix with more than 2000 (16384) elements: Slowness may result.\n",
      "Orthogonal initializer is being called on a matrix with more than 2000 (16384) elements: Slowness may result.\n",
      "Orthogonal initializer is being called on a matrix with more than 2000 (16384) elements: Slowness may result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output shape              Param #   \n",
      "=================================================================\n",
      "input5 (InputLayer)          [null,140]                0         \n",
      "_________________________________________________________________\n",
      "embedding_Embedding5 (Embedd [null,140,219]            47961     \n",
      "_________________________________________________________________\n",
      "bidirectional_Bidirectional9 [null,140,128]            145408    \n",
      "_________________________________________________________________\n",
      "bidirectional_Bidirectional1 [null,140,128]            98816     \n",
      "_________________________________________________________________\n",
      "flatten_Flatten2 (Flatten)   [null,17920]              0         \n",
      "_________________________________________________________________\n",
      "dense_Dense4 (Dense)         [null,2]                  35842     \n",
      "=================================================================\n",
      "Total params: 328027\n",
      "Trainable params: 280066\n",
      "Non-trainable params: 47961\n",
      "_________________________________________________________________\n",
      "1/12628 loss: 0.6970651149749756    accuracy: 0.3349999487400055\n",
      "negative: 0.39026233553886414    positive: 0.6097375750541687\n",
      "51/12628 loss: 0.5808426737785339    accuracy: 0.7199999690055847\n",
      "negative: 0.450821191072464    positive: 0.5491787791252136\n",
      "101/12628 loss: 0.5366352796554565    accuracy: 0.7599999904632568\n",
      "negative: 0.3710169792175293    positive: 0.6289829611778259\n",
      "151/12628 loss: 0.507841944694519    accuracy: 0.7799999713897705\n",
      "negative: 0.397342324256897    positive: 0.602657675743103\n",
      "201/12628 loss: 0.565376877784729    accuracy: 0.7299998998641968\n",
      "negative: 0.3155762255191803    positive: 0.6844237446784973\n",
      "251/12628 loss: 0.6150363087654114    accuracy: 0.6849998831748962\n",
      "negative: 0.258805513381958    positive: 0.741194486618042\n",
      "301/12628 loss: 0.5238387584686279    accuracy: 0.7699999809265137\n",
      "negative: 0.31987708806991577    positive: 0.6801229119300842\n"
     ]
    }
   ],
   "source": [
    "trainClassification(createModel({'type': 'classification', 'bidi': true, 'lstm_units': 64}), 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-07T17:57:54.535Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Orthogonal initializer is being called on a matrix with more than 2000 (16384) elements: Slowness may result.\n",
      "Orthogonal initializer is being called on a matrix with more than 2000 (16384) elements: Slowness may result.\n",
      "Orthogonal initializer is being called on a matrix with more than 2000 (16384) elements: Slowness may result.\n",
      "Orthogonal initializer is being called on a matrix with more than 2000 (16384) elements: Slowness may result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output shape         Param #     Receives inputs                  \n",
      "==================================================================================================\n",
      "input1 (InputLayer)             [null,140]           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_Embedding1 (Embedding [null,140,219]       47961       input1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_Bidirectional1 (B [null,140,128]       145408      embedding_Embedding1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_Bidirectional2 (B [null,140,128]       98816       bidirectional_Bidirectional1[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_Dense1 (Dense)            [null,140,1]         129         bidirectional_Bidirectional2[0][0\n",
      "__________________________________________________________________________________________________\n",
      "flatten_Flatten1 (Flatten)      [null,140]           0           dense_Dense1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_Activation1 (Activat [null,140]           0           flatten_Flatten1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_RepeatVector1 (Re [null,128,140]       0           activation_Activation1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "permute_Permute1 (Permute)      [null,140,128]       0           repeat_vector_RepeatVector1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "multiply_Multiply1 (Multiply)   [null,140,128]       0           bidirectional_Bidirectional2[0][0\n",
      "                                                                 permute_Permute1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_Global [null,128]           0           multiply_Multiply1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_Dense2 (Dense)            [null,2]             258         global_average_pooling1d_GlobalAv\n",
      "==================================================================================================\n",
      "Total params: 292572\n",
      "Trainable params: 244611\n",
      "Non-trainable params: 47961\n",
      "__________________________________________________________________________________________________\n",
      "1/12628 loss: 0.6932795643806458    accuracy: 0.28999996185302734\n",
      "negative: 0.49906718730926514    positive: 0.5009328126907349\n"
     ]
    }
   ],
   "source": [
    "trainClassification(createModel({'type': 'classification', 'bidi': true, 'attention': true, 'lstm_units': 64}), 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Javascript (Node.js)",
   "language": "javascript",
   "name": "javascript"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "10.13.0"
  },
  "notify_time": "10"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
